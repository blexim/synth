% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass[a4paper]{llncs}
%

\usepackage{listings}
\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows, automata, shapes}


%\usepackage{makeidx}  % allows for indexgeneration
%
\title{{\sc Kalashnikov}: Fully Automatic Loop Free Program Synthesis by Bounded Model Checking}
\author{Matt Lewis}
\institute{Oxford University}

\newenvironment{keywords}{
       \list{}{\advance\topsep by0.35cm\relax\small
       \leftmargin=0cm
       \labelwidth=0.35cm
       \listparindent=0.35cm
       \itemindent\listparindent
       \rightmargin\leftmargin}\item[\hskip\labelsep
                                     \bfseries Keywords:]}
     {\endlist}


\begin{document}
%
\maketitle
%
\pagestyle{headings}  % switches on printing of running heads

\abstract
We present a method for synthesizing loop free programs using a combination of
bounded model checking and explicit enumeration.  The user provides a specification
in plain C and is not required to interact further with the synthesis processs.
In particular the user need not tell the synthesizer which program components to
use.

We demonstrate the effectiveness of our method by synthesizing several tricky
bitvector programs and a selection of floating point programs.


\begin{keywords}
 Program synthesis, bitvectors, bounded model checking, CBMC,
 floating point.
\end{keywords}

\section{Introduction}

\section{Related Work}

\section{Basic Synthesis Algorithm}

\subsection{The Abstract Algorithm}

Our task is to find a program which satisfies some specifcation.  We can formalize this
notion as follows: fix an input set $I$ and an output set $O$.  Specifications $\sigma$
are then relations and programs $P$ are computable functions:

$$ \sigma \subseteq I \times O $$
$$ P : I \rightarrow O$$

The synthesis problem is then to determine the validity of the following formula, and
to find a witness $P$ if the following second-order formula is valid:

$$\exists P . \forall x \in I . \sigma(x, P(x))$$

Depending on the logic needed to express this formula the synthesis problem may
be decidable, semi-decidable or undecidable.  For many interesting logics, checking
the validity of a second order formula, such as the synthesis formula, is undecidable.
Even for logics in which the problem is decidable, the quantifier alternation means that
we do not have efficient decision procedures to verify our formula, although efficient
methods may exist for solving problems with only one quantifier.  In particular
if we are working in the logic of propositional satisfiability, we have SAT solvers
which are efficient-in-practice at solving queries with a single quantifier.
The corresponding problem with one level of quantifier alternation (2-QBF) is
complete for $NP^{NP}$, and current 2-QBF solvers are much less efficient than SAT solvers.

When the first-order fragment of the logic is efficiently decidable and $P$ can be
expressed as a ground term of the logic, we can use a refinement loop to check the
validity of the synthesis formula as shown in Fig.~\ref{fig:abstract-refinement} and
Fig.~\ref{fig:abstract-refinement-code}.  The algorithm is divided into two
components: {\sc synth} and {\sc verif}.  We track a finite (small) set of inputs
and {\sc synth} synthesizes a candidate program which is correct on just those inputs.
{\sc Verif} then tries to verify the candidate program by searching for an input on
which the candidate program does not satisfy the specification.  If no such input
can be found, the candidate program is correct.  Otherwise, the new input is added
to the set of inputs we must check in {\sc synth}.  If the input set is finite, this
procedure is guaranteed to terminate.

\begin{figure}
 \centering
 \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,
 semithick, initial text=]

  \matrix[nodes={draw, fill=none, scale=1, shape=rectangle, minimum height=1cm, minimum width=1.5cm},
          row sep=2cm, column sep=4cm] {
   \node (synth) {Synthesize};
   &
   \node (verif) {Verify}; \\
   \node[draw=none] {};
   &
   \node[ellipse] (done) {Done}; \\
  };

   \path
    (synth) edge [bend left] node {Candidate program} (verif)
    (verif) edge [bend left] node {Counterexample input} (synth)
    (verif) edge node {Valid} (done);
 \end{tikzpicture}

 \label{fig:abstract-refinement}
 \caption{Abstract synthesis refinement loop}
\end{figure}

\begin{figure}
\centering
\begin{minipage}{0.9\textwidth}
 \begin{lstlisting}[mathescape,language=C]
function synth(inputs) {
  $(i_1, \ldots, i_N)$ = inputs
  query := $\exists P . \sigma(i_1, P(i_1)) \land \ldots \land \sigma(i_N, P(i_N))$
  result := decide(query)
  
  if (result.satisfiable) {
    return result.model
  } else {
    return unsatisfiable
  }
}

function verif(P) {
  query := $\exists x . \lnot \sigma(x, P(x))$
  result := decide(query)
  
  if (result.satisfiable) {
    return result.model
  } else {
    return valid
  }
}

function refinement_loop() {
  inputs := $\emptyset$

  while (true) {
    candidate := synth(inputs)

    if (candidate = unsatisfiable) {
      return unsatisfiable
    }

    res := verif(candidate)

    if (res = valid) {
      return candidate
    } else {
      inputs := inputs $\cup$ res
    }
  }
 \end{lstlisting}
 \end{minipage}

 \label{fig:abstract-refinement-code}
 \caption{Abstract refinement algorithm}
\end{figure}

\subsection{The Concrete Algorithm for Bitvector Programs}

One area in which program synthesis can shine is in producing very small,
intricate programs that manipulate bitvectors.  An example of such a program
is shown in Fig.~\ref{fig:bitvector-program}.  This program takes a machine word
as input and clears every bit except for the least significant bit that was set.
Even though this program is extremely short, it is fairly difficult for a human
to see what it does.  It is even more difficult for a human to come up with such
minimal code -- a natural solution for this problem might be to use a loop which
iterates over all of the bits in the word.  The program is so concise because it
takes advantage of the low level details of the machine, such as the fact that
signed integers are stored in two's complement form.

\begin{figure}
\centering
\begin{minipage}{0.45\linewidth}
 \begin{lstlisting}[language=C]
int isolate_lsb(int x) {
  return x & -x;
}
 \end{lstlisting}
\end{minipage}
\begin{minipage}{0.45\linewidth}
 
Example:

\hrule

\begin{tabular}{llcccccccc}
 x       & = & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 0 \\
 -x      & = & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 \\
 x \& -x & = & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0
\end{tabular}
\end{minipage}

 \label{fig:bitvector-program}
 \caption{A tricky bitvector program}
\end{figure}

To synthesize tricky bitvector programs like this, it is natural for us to work
in the logic of quantifer free propositional formulae and to use a SAT solver as
the decision procedure.  However, we propose a slightly different tack, which is
to use a decidable fragment of C as our underlying logic.  The fragment we use
has the following restrictions:

\begin{itemize}
 \item All loops and recursive function calls must terminate after a constant number
 of iterations.  This constant must be statically inferrable.
 \item Arrays must all be statically allocated with a constant size.
\end{itemize}

Other than this any C constructs can be used, which still leaves a very expressive
language with the nice property that safety is decidable using a single query to a
bounded model checker.  We will refer to this fragment as C-.  To instantiate the
abstract synthesis algorithm in C- we must express $I, O, \sigma$ and $P$ in C-,
then ensure that we can express the validity of the synthesis formula as a safety
property of the resulting C- program.

Our encoding is the following:

\begin{itemize}
 \item $I$ is the type \verb|int[N]|.
 \item $O$ is the type \verb|int[M]|.
 \item $\sigma$ is a function with signature
 \verb|int check(int in[N], int out[M])|. This function is the only component supplied
 by the user.
 \item $P$ is written in a simple RISC language $\mathcal{L}$.  Programs in $\mathcal{L}$
 have the type \verb|prog_t|.
 \item We supply an interpreter for $\mathcal{L}$ which is written in C-.  The signature
 of this interpreter is \\
 \verb|void exec(prog_t p, int in[N], int out[M])|.
\end{itemize}

\begin{figure}
\centering
\begin{minipage}{0.65\textwidth}
\setlength{\tabcolsep}{16pt}
Integer arithmetic instructions:

\begin{tabular}{lll}
 \verb|add a b| & \verb|sub a b| & \verb|mul a b| \\
 \verb|div a b| & \verb|neg a|
\end{tabular}
\bigskip

Bitwise logical and shift instructions:

\begin{tabular}{lll}
 \verb|and  a b| & \verb|or   a b| & \verb|xor  a b| \\
 \verb|ashr a b| & \verb|lshr a b| & \verb|not  a|
\end{tabular}
\bigskip

Unsigned and signed comparison instructions:

\begin{tabular}{lll}
 \verb|le  a b| & \verb|lt  a b| & \verb|sle  a b| \\
 \verb|slt a b|
\end{tabular}
\end{minipage}

 \label{fig:l-language}
 \caption{The language $\mathcal{L}$}
\end{figure}

We must now express the {\sc synth} and {\sc verif} formulae as safety properties
of C- programs, which is shown in Fig.~\ref{fig:c-synthverif}.

\begin{figure}
\centering
\begin{minipage}{0.45\textwidth}
\begin{lstlisting}[language=C++]
void synth() {
  prog_t p = nondet();
  int in[N], out[M];
  
  in = test1;
  exec(p, in, out);
  assume(check(in, out));
  
  ...
  
  in = testN;
  exec(p, in, out);
  assume(check(in, out));
  
  assert(false);
}
\end{lstlisting}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{lstlisting}[language=C]
void verif(prog_t p) {
  int in[N] = nondet();
  int out[M];

  exec(p, in, out);
  assert(check(in, out));
}
\end{lstlisting}
\end{minipage}

 \label{fig:c-synthverif}
 \caption{The {\sc synth} and {\sc verif} formulae expressed as a C- program}
\end{figure}

\section{Optimizations}

By far the dominant factor in the total runtime of the synthesis algorithm is
the runtime of {\sc synth}.  It is not entirely surprising that this runtime
is strongly related to the size of the SAT problem generated by CBMC.  The size
of the SAT problem is influenced by the number of bits needed to encode $P$,
and by the number of bits needed to encode a run of the $\mathcal{L}$-interpreter.
The connection to Kolmogorov complexity is fairly obvious.

The most successful optimizations we found were related to making fragments of
$\mathcal{L}$ and searching progressively larger fragments until a program
could be found.

\subsection{Limited Program Length}
The simplest restriction on programs is to only consider programs of a certain length.
We start by considering programs consisting of one instruction and progressively
increase this number.

\subsection{Word Width}
It is often the case that a program which satisfies the specification
will continue to satisfy it regardless of the word width of the machine it is
impemented on.  For example the program in Fig.~\ref{fig:bitvector-program}
isolates the least significant bit of an 8-bit word, but it will also
isolate the least significant bit of a 32-bit or indeed any sized word.

Because of this, we can often find a program that is correct for a large
word size by synthesizing a program for a machine with a smaller word size.
This reduces the size of all the constants in the program, the search space
of possible test inputs and also the size of all the state variables internal
to the $\mathcal{L}$-interpreter.

The only wrinkle here is that sometimes a program we synthesize will contain
constants.  If we have synthesized a program for a machine with $k$-bit words,
the constants in the program will be $k$ bits wide.  To generalize the program
to a $n$-bit machine (with $n > k$), we need some way of deriving $n$-bit
wide numbers from $k$-bit ones.  We have several strategies for this and
just try each in turn.  To generalize a number $x$ we use the following
rules:

\begin{itemize}
 \item If $x = k_k$, generalize to $n_n$.
 \item If $x = k-1_k$, generalize to $n-1_n$.
 \item If $x = k+1_k$, generalize to $n+1_n$.
 \item Generalize $x_k$ to $x_n$.
 \item Generalize $x_k$ to $x_k \cdot 0_{n-k}$.
 \item Generalize $x_k$ to $x_k^{n / k}$.
\end{itemize}


\subsection{Separate Constants}
$\mathcal{L}$ is a SSA, three address instruction set.  Destination registers
are implicit and a fresh register exists for each instruction to write its
output to.  A natural way to encode $\mathcal{L}$ instructions is to have an
opcode and two operands.  The opcode selects which instruction type is being
executed and the operands specify which registers should be operated on.
We might allow the operands to be either a register (i.e. a program argument
or the result of a previous instruction), or an immediate constant.

Each opcode requires $\log_2 I$ bits to encode, where $I$ is the number
of instruction types in $\mathcal{L}$.  Each operand can be encoded using
$\log_2 w$ bits, where $w$ is the $\mathcal{L}$-machine word, plus one
bit to specify whether the operand is a register name or an immediate constant.
One instruction can therefore be encoded using $\log_2 I + 2w + 2$ bits.
For an $n$ instruction program, we need $$n \log_2 I + 2nw + 2n$$ bits to encode
the entire program.

If we instead limit the number of constants that can appear in the program,
our operands can be encoded using fewer bits.  For an $n$ instruction program
using $k$ constants and taking $a$ arguments as inputs, each operand can refer
to a program argument, the result of a previous instruction or a constant.
This can be encoded using $\log_2 (k+a+n-1)$ bits, which means each instruction
can be encoded in $\log_2 I + \log_2 (k + a + n - 1)$ and the full program
needs $$n \log_2 I + n \log_2 (k + a + n - 1) + kw$$ bits to encode.

To give an example, $\mathcal{L}$ has 15 instruction types, so each opcode is 4 bits.
For a 10 instruction program over 1 argument, using 2 constants on a 32-bit word
machine the first encoding requires $10 * (4 + 32 + 1 + 32 + 1) = 700$ bits.
Using the second encoding, each operand can be represented using
$\log_2 (2 + 1 + 10 - 1) = 4$ bits, and the entire program requires 184 bits.
This is a substantial reduction in size and when the required program requires
few constants this can lead to a very significat speed up.

As with program length we can progressively increase the number of constants in
our program.  We start by trying to synthesize a program with no constants, then
if that fails we try to synthesize using one constant and so on.

\subsection{Explicit Search}
When we are working on an $\mathcal{L}$-machine with a small word size and
a compact program encoding, our state space is very small.
We can quickly check the validity of the {\sc synth} and {\sc verif} formulae
by simply enumerating all of the inputs and programs, then explicitly
executing each program on each input.

Since everything in our system is written in C, we can add a simple loop
to enumerate all programs and inputs, then compile the code using {\sc gcc}.
The resulting binary will explicitly search for programs satisfying the {\sc synth}
formula, or for counterexamples satisfying the {\sc verif} formula.
For small state spaces, these explicit search binaries are often faster
than symbolically evaluating the {\sc synth} and {\sc verif} formulae with
{\sc CBMC}.

\subsection{Remove nops}
Many instructions in $\mathcal{L}$ are nops that do not doing anything.
For example the instruction \verb|add x 0| does nothing.  Such instructions
can be removed from any program they appear in to leave a semantically
equivalent, but shorter, program.  We can therefore be sure that nops
will never appear in any minimal program.  By adding constraints saying that
each instruction is not a nop, we can help the underlying SAT solver's
search, which reduces the runtime of the overall procedure.

\subsection{Symmetry Reduction}
There are many instructions that are equivalent to each other.  For example,
\verb|add x y| is equivalent to \verb|add y x| -- any program containing
one instruction could have it replaced by the other instruction and
keep the same semantics.  We choose a single canonical instruction to
represent all instructions in a particular equivalence class, then add
constraints saying that no non-canonical instructions appear in the program.

Our rules for excluding non-canonical instructions are:

\begin{itemize}
 \item For commutative operations, the first operand is smaller than the second.
 \item For unary operations, the second (unused) operand is always 0.
 \item No instruction may have two constant operands.
 \item All program constants are distinct.
\end{itemize}

As with the nop constraints, these additional constraints do increase the
size of the resulting SAT instance, but this still ends up as a win in
terms of runtime.

\subsubsection{Implementing $\mathcal{L}$ With A Stack Machine}
Three-address code tends to be less compact than code written for a stack
machine.  It therefore would seem reasonable to use a stack machine to
execute $\mathcal{L}$ programs, since we would expect to see smaller programs
(in terms of bits used in the encoding), leading to smaller SAT instances
and faster runtimes overall.

Unfortunately we saw quite the opposite effect -- runtimes when implementing the
$\mathcal{L}$-interpreter as a stack machine were much higher than the three-address
code implementation.  We believe the reason for this
effect is that the internal state of a stack based interpreter is much more complex
to analyse than an interpreter for three-address code.  This is because a
three-address code interpreter can be implemented using a single array whose cells are
each written to exactly once (each cell corresponds to a register in the program).
By contrast a stack machine interpreter has a stack as its central data structure.
It is natural to implement this as an array, but the array's contents are accessed
in an unpredictable, non-uniform manner as the stack pointer increases and decreases
over the lifetime of the program.  This leads to a \emph{larger} SAT
isntance and drastically slows down the operation of the solver.


\section{Floating Point}

\section{Experimental Results}

\section{Conclusion}

\end{document}
