% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass[a4paper]{llncs}
%

\usepackage{listings}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{pgf}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{pifont}
\usepackage{multicol}

\usepackage{tikz}
\usetikzlibrary{arrows, automata, shapes}

\newtheorem{defn}{Definition}

\algrenewcommand\algorithmicrequire{\textbf{Precondition:}}
\algrenewcommand\algorithmicensure{\textbf{Postcondition:}}

\newcommand{\xmark}{\ding{55}}

\newcommand{\newC}{C$^-$\xspace}
\newcommand*\Let[2]{\State #1 $\gets$ #2}

\newcommand{\bv}[2]{\mathcal{BV}(#1, #2)}

\iffalse
\lstset{emph={
  assume, assert}, emphstyle=\bfseries}
\fi
  
%\usepackage{makeidx}  % allows for indexgeneration
%
\title{Fully Automatic Loop-free Program Synthesis by Bounded Model Checking}
\author{Daniel Kroening \and Matt Lewis}
\institute{University of Oxford}

\newenvironment{keywords}{
       \list{}{\advance\topsep by0.35cm\relax\small
       \leftmargin=0cm
       \labelwidth=0.35cm
       \listparindent=0.35cm
       \itemindent\listparindent
       \rightmargin\leftmargin}\item[\hskip\labelsep
                                     \bfseries Keywords:]}
     {\endlist}

\makeatletter
\pgfdeclareshape{datastore}{
  \inheritsavedanchors[from=rectangle]
  \inheritanchorborder[from=rectangle]
  \inheritanchor[from=rectangle]{center}
  \inheritanchor[from=rectangle]{base}
  \inheritanchor[from=rectangle]{north}
  \inheritanchor[from=rectangle]{north east}
  \inheritanchor[from=rectangle]{east}
  \inheritanchor[from=rectangle]{south east}
  \inheritanchor[from=rectangle]{south}
  \inheritanchor[from=rectangle]{south west}
  \inheritanchor[from=rectangle]{west}
  \inheritanchor[from=rectangle]{north west}
  \backgroundpath{
    %  store lower right in xa/ya and upper right in xb/yb
    \southwest \pgf@xa=\pgf@x \pgf@ya=\pgf@y
    \northeast \pgf@xb=\pgf@x \pgf@yb=\pgf@y
    \pgfpathmoveto{\pgfpoint{\pgf@xa}{\pgf@ya}}
    \pgfpathlineto{\pgfpoint{\pgf@xb}{\pgf@ya}}
    \pgfpathmoveto{\pgfpoint{\pgf@xa}{\pgf@yb}}
    \pgfpathlineto{\pgfpoint{\pgf@xb}{\pgf@yb}}
 }
}
\makeatother



\begin{document}
%
\maketitle
%
\pagestyle{headings}  % switches on printing of running heads

\begin{abstract}

We present a simple-yet-effective method for synthesising loop-free programs
from specifications, using a novel combination of Bounded Model Checking and
explicit-state Model Checking.  The user provides a specification in plain
C, after which the synthesis process is fully automatic and is guaranteed to
synthesise a minimal program.

We demonstrate the effectiveness of our method by synthesising several subtle
programs acting on machine integers, de-obfuscating complex C code and
synthesising a previously un-synthesised floating-point program.
\end{abstract}


\begin{keywords}
 Program synthesis, bitvectors, bounded model checking, CBMC,
 floating point.
\end{keywords}

\section{Introduction}

Program synthesis is the mechanised construction of software that provably
satisfies a given specification.  Synthesis tools promise to relieve the
programmer from thinking about \emph{how} the problem is to be solved;
instead, the programmer only provides a compact description of \emph{what}
is to be achieved.  Foundational research in this area has been
exceptionally fruitful, beginning with Alonzo Church's work on the
\emph{Circuit Synthesis Problem} in the sixties~\cite{church-synth}.
Algorithmic approaches to the problem have frequently been connected to
automated theorem proving~\cite{manna-waldinger,proof-planning}.
Recent developments include an application of Craig interpolation
to synthesis~\cite{synth-interpolants}.

In this paper, we present a new method that brings the promise of
automating programming closer to application.  We make pragmatic
restrictions to the general synthesis problem in order to arrive at an
effective solution.  To this end, we focus on the problem of synthesising
loop-free programs over machine integers and floating-point numbers from a
specification given as program fragment.  Our exploration of the program
space ensures that our programs are minimal in size.

Our work represents a substantial refinement of {\sc brahma}~\cite{brahma},
which is the closest work to ours.  {\sc Brahma} also focuses on
synthesising loop-free machine-integer programs.  In~\cite{brahma}, the
authors characterise the program synthesis problem as a second-order
$\exists \forall$ formula and use a variation of the CEGIS algorithm,
introduced in~\cite{lezama-thesis}, for solving
these formulae.  This algorithm reduces the satisfiability of the $\exists
\forall$ formula to the satisfiability of several first-order formulae, each
of which can be decided using an off-the-shelf SMT solver.  {\sc Brahma}
takes as input a specification (given as an SMT formula) and a library of
components (one component is more or less a single machine instruction).  It
then finds a permutation of the component library that satisfies the
specification.  In the
event that the library does not contain enough components to synthesise a
correct program, the user is told that the library is too small and
must add more components until the synthesis succeeds.  Each round of
synthesis amounts to solving an $\exists \forall$ formula that is quadratic
in the size of the component library.

We require the user to supply a program specification written in C, but then
ask nothing more of her.  We parametrise the space of programs in such a way
that it can be explored automatically, rather than asking a human for hints.
Our program encoding generates an $\exists
\forall$ formula that is \emph{linear} in the length of the shortest program
satisfying the user's specification.  This formula is then checked for satisfiability
using the CEGIS algorithm.  In contrast, the program generated by {\sc
brahma} can be no longer than the size of the library, as the synthesis
outcome is a permutation of the component library.  Our synthesis
formulae are therefore asymptotically smaller than {\sc brahma}'s.

\paragraph{Related Work}

Another successful approach to program synthesis has been \emph{program
sketching}, in particular the {\sc sketch} tool~\cite{lezama-thesis,sketch,modular-sketch}.  This method
requires the programmer to provide a \emph{program sketch}, written in a
C-like language, together with either a specification (written in the same
language) or a suite of unit tests.  The sketch is a skeleton program
containing much of the high-level structure of the final program, but with
several ``holes'', which {\sc sketch} fills in to meet the specification.
Despite the many similarities, our problem domains are quite different:
sketching is able to make use of human insight to synthesise fairly large
programs, whereas we are optimised for synthesising small (but intricate)
programs without guidance.  A sketching-like approach to optimising
fixed point arithmetic code was described in~\cite{eldib-wang}.  This
work is similar in goal to our floating-point work, but is more restricted
in scope in that it only tries to minimise register widths for embedded
systems using fixed point arithmetic.

Program synthesis is closely related to
superoptimisation, which is the problem of taking a
segment of code, usually written in C, and finding a minimal program which
has the same functionality.  The {\sc aha}~\cite{aha-paper} tool, the GNU
superoptimiser~\cite{gnu-superoptimiser} and TOAST~\cite{toast} all perform superoptimisation of C
programs by enumerating possible programs and testing them on suitable
inputs.  This search can be compiled down to native code using an optimising
compiler, so for short programs the method is extremely fast.  However, this
method is impractical for synthesising longer programs because of the
exponential blowup of the search space.  We incorporate the core idea of
this technique by using explicit-state model checking in parallel with
symbolic model checking.

\paragraph{Technical Contributions}
Our approach generates an $\exists \forall$ formula that is linear
in the size of the synthesised program.  This improves over the
encoding of~\cite{brahma}, which generates a $\exists \forall$ formula
that is quadratic in the size of the synthesised program.

We introduce a novel parametrisation of the programming language
used to express our synthesised programs.
This parametrisation allows us to efficiently explore the program space
without relying on human guidance and also ensures that our programs
are of minimal length.

Our tool is the first we are aware of that is able to effectively
synthesise floating-point programs.  We demonstrate this by
synthesising {\sc Fast2Sum} using Knuth's {\sc 2Sum}~\cite{taocp2} as
a specification.

\paragraph{Outline} We describe the abstract synthesis algorithm in
Sec.~\ref{sec:abstract-algorithm} and give details of our new algorithm in
Sec.~\ref{sec:concrete-algorithm}.  We use a fragment of the C programming language
as our underlying logic, which is elaborated in Sec.~\ref{sec:logic}. We
summarise the results of our experiments in Sec.~\ref{sec:experiments}.

\section{The Basic Synthesis Algorithm}
\label{sec:abstract-algorithm}

\subsection{The Synthesis Formula}
\label{sec:abstract-formula}

Our task is to find a program that satisfies some specification.  We can formalise this
notion as follows: fix an input set $I$ and an output set $O$.  Specifications $\sigma$
relate inputs and outputs, and programs $P$ map an input to an output:
%
$$ \sigma \subseteq I \times O $$
$$ P : I \rightarrow O$$

The synthesis problem is then to determine the validity of the following
second-order formula, and if so, to find a suitable witness program $P$:
%
$$\exists P .\, \forall x \in I.\, \sigma(x, P(x))$$

Depending on the logic needed to express this formula the synthesis problem
may be decidable, semi-decidable or undecidable.  For many interesting
logics, checking the validity of a second-order formula is undecidable. 
Even for logics in which the problem is decidable, the quantifier
alternation means that we may not have an efficient decision procedure for
the formula above, despite efficient methods existing for problems
with only one quantifier.  In the case of propositional
satisfiability, we have SAT solvers which are efficient-in-practice at
solving queries with a single quantifier.  The corresponding problem with
one level of quantifier alternation (2-QBF) is complete for $NP^{NP}$, and
current 2-QBF solvers are much less efficient in practice than SAT solvers.

\subsection{CEGIS}

When the quantifier free first-order fragment of the logic is efficiently decidable and
$P$ can be expressed as a ground term of the logic, we can use
Counterexample Guided Inductive Synthesis (CEGIS)~\cite{lezama-thesis,sketch} to
check the validity of the synthesis formula.  The core of the CEGIS algorithm is
a refinement loop to check the
validity of the synthesis formula as shown in Fig.~\ref{fig:abstract-refinement} and
Algorithm~\ref{fig:abstract-refinement-code}.  The algorithm is divided into two
components: {\sc synth} and {\sc verif}.  We track a finite (small) subset of test inputs
and {\sc synth} synthesises a candidate program that is correct on just those tests.
{\sc Verif} then tries to verify the candidate program by searching for an input on
which the candidate program does not satisfy the specification.  If no such input
can be found, the candidate program is correct.  Otherwise, the new input is added
to the set we must check in {\sc synth}.  If the input set is finite, this
procedure is guaranteed to terminate.

\begin{algorithm}
 \caption{Abstract refinement algorithm
 \label{fig:abstract-refinement-code}}

 \begin{multicols}{2}
 \begin{algorithmic}[1]
\Statex
\Function{synth}{inputs}
  \Let{$(i_1, \ldots, i_N)$}{inputs}
  \Let{query}{$\exists P . \sigma(i_1, P(i_1)) \land \ldots \land \sigma(i_N, P(i_N))$}
  \Let{result}{decide(query)}
  \If{result.satisfiable}
    \State \Return{result.model}
  \Else
    \State \Return{unsatisfiable}
  \EndIf
\EndFunction
\Statex
\Function{verif}{P}
  \Let{query}{$\exists x . \lnot \sigma(x, P(x))$}
  \Let{result}{decide(query)}
  \If{result.satisfiable}
    \State \Return{result.model}
  \Else
    \State \Return{valid}
  \EndIf
\EndFunction
\columnbreak
\Statex
\Function{refinement loop}{}
  \Let{inputs}{$\emptyset$}
  \Loop
    \Let{candidate}{\Call{synth}{inputs}}
    \If{candidate = UNSAT}
      \State \Return{unsatisfiable}
    \EndIf
    \Let{res}{\Call{verif}{candidate}}
    \If{res = valid}
      \State \Return{candidate}
    \Else
      \Let{inputs}{inputs $\cup$ res}
    \EndIf
  \EndLoop
\EndFunction
 \end{algorithmic}
 \end{multicols}
\end{algorithm}


\begin{figure}
 \centering
 \begin{tikzpicture}[scale=0.5,->,>=stealth',shorten >=1pt,auto,
 semithick, initial text=]

  \matrix[nodes={draw, fill=none, scale=1, shape=rectangle, minimum height=1cm, minimum width=1.5cm},
          row sep=2cm, column sep=3cm] {
   \node (synth) {Synthesise};
   &
   \node (verif) {Verify}; %\\
   %\node[draw=none] {};
   &
   \node[ellipse] (done) {Done}; \\
  };

   \path
    (synth) edge [bend left] node {Candidate program} (verif)
    (verif) edge [bend left] node {Counterexample input} (synth)
    (verif) edge node {Valid} (done);
 \end{tikzpicture}
 
 \caption{Abstract synthesis refinement loop
 \label{fig:abstract-refinement}}
\end{figure}


\section{The Concrete Algorithm for Bitvector Programs}
\label{sec:concrete-algorithm}

One area in which program synthesis can shine is in producing very small,
intricate programs that manipulate bitvectors.  An example of such a program
is given in Fig.~\ref{fig:bitvector-program}.  This program takes a machine word
as input and clears every bit except for the least significant bit that was set.
Even though this program is extremely short, it is fairly difficult for a human
to see what it does.  It is even more difficult for a human to come up with such
minimal code.  The program is so concise because it
takes advantage of the low-level details of the machine, such as the fact that
signed integers are stored in two's complement form.

\begin{figure}
\centering
\begin{minipage}{0.45\linewidth}
 \begin{lstlisting}[language=C]
int isolate_lsb(int x) {
  return x & -x;
}
 \end{lstlisting}
\end{minipage}
\begin{minipage}{0.45\linewidth}
 
Example:

\hrule

\begin{tabular}{llcccccccc}
 x       & = & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 0 \\
 -x      & = & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 \\
 x \& -x & = & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0
\end{tabular}
\end{minipage}


 \caption{A tricky bitvector program}
  \label{fig:bitvector-program}
\end{figure}


To synthesise tricky bitvector programs like this, it is natural for us to
work in the logic of quantifier-free propositional formulae and to use a
propositional SAT or SMT-$\mathcal{BV}$ solver as the decision procedure. 
However we propose a slightly different tack, which is to use a decidable
fragment of C as a ``high level'' logic.


\subsection{\newC}
\label{sec:logic}

We will now describe the logic we use to express our synthesis formula.
The logic is a subset of C that we call \newC.  The characteristic property of a
\newC  program is that safety can be decided for
it using a single query to a Bounded Model Checker.  A \newC program is
just a C program with the following syntactic restrictions:
 all loops in the program must have a constant bound;
 all recursion in the program must be limited to a constant depth;
 all arrays must be statically allocated (i.e. not using \texttt{malloc}),
 and be of constant size.
Additionally, \newC programs may use nondeterministic values, assumptions
and arbitrary-width types.

\iffalse
Two example \newC programs are shown 
in Fig.~\ref{fig:c-}.

\begin{figure}
\begin{minipage}[scale=0.8]{0.45\linewidth}
 \begin{lstlisting}[language=c]
int count_bits(int x) {
  int i, ret = 0;
  
  for (i = 0; i < 32; i++)
    if (x & (1 << i))
      ret++;
  
  return ret;
}
 \end{lstlisting}
\end{minipage}
\begin{minipage}{0.54\linewidth}
 \begin{lstlisting}[language=C]
int common_factor(int A[10]) {
  int i, factor = nondet();

  for (i = 0; i < 10; i++)
    assume((A[i] % factor) == 0);

  assume(factor > 1);
  return factor;
}

 \end{lstlisting}
\end{minipage}

 \caption{Two \newC programs}
 \label{fig:c-}

\end{figure}
\fi

Since each loop is bounded by a constant, and each recursive function call is
limited to a constant depth, a \newC program necessarily terminates and in
fact does so in $O(1)$ time.  If we call the largest loop bound~$k$, then
a Bounded Model Checker with an unrolling bound of $k$ will be a complete
decision procedure for the safety of the program.  For a \newC program of
size $l$ and with largest loop bound~$k$, a Bounded Model Checker will
create a SAT problem of size $O(lk)$.  Conversely, a SAT problem
of size $s$ can be converted trivially into a loop-free \newC program
of size $O(s)$.  The safety problem for \newC is therefore NP-complete,
which means it can be decided fairly efficiently for many practical
instances.


\subsection{Encoding into \newC}
To instantiate the abstract synthesis algorithm in \newC we must express $I, O,
\sigma$ and $P$ in \newC, then ensure that we can express the validity of the
synthesis formula as a safety property of the resulting \newC program.

Our encoding is the following:
%
\begin{itemize}
 \item $I$ is the set of $N$-tuples of 32-bit bitvectors.  This is written in \newC as the type \verb|int[N]|.
 \item $O$ is the set of $M$-tuples of 32-bit bitvectors, which is written in \newC as the type \verb|int[M]|.
 \item $\sigma$ is a pure function with type $I \times O \rightarrow \mathrm{Bool}$.  The \newC signature of this function is
 \verb|int check(int in[N], int out[M])|. This function is the only component supplied
 by the user.
 \item $P$ is written in a simple RISC-like language $\mathcal{L}$, whose syntax is given in Fig.~\ref{fig:l-language}.  Programs in $\mathcal{L}$
 have type $I \rightarrow O$ and
 are represented in \newC as objects of type \verb|prog_t|, shown in Fig.~\ref{fig:c-l-encoding}.
 \item We supply an interpreter for $\mathcal{L}$ which is written in \newC.  The type of
 this interpreter is $(I \rightarrow O) \times I \rightarrow O$ and the \newC signature is \\
 \verb|void exec(prog_t p, int in[N], int out[M])|.  Here, \verb|out| is an output parameter.
\end{itemize}

\begin{figure}
{\small
\begin{center}
\setlength{\tabcolsep}{16pt}
Integer arithmetic instructions:

\begin{tabular}{llll}
 \verb|add a b| & \verb|sub a b| & \verb|mul a b| & \verb|div a b| \\
 \verb|neg a| & & &
\end{tabular}

\medskip

Bitwise logical and shift instructions:

\begin{tabular}{llll}
 \verb|and  a b| & \verb|or a b| & \verb|xor a b| & \verb|ashr a b| \\
 \verb|lshr a b| & \verb|not  a| & &
\end{tabular}

\medskip

Unsigned and signed comparison instructions:

\begin{tabular}{llll}
 \verb|le  a b| & \verb|lt  a b| & \verb|sle  a b| & \verb|slt a b|
\end{tabular}
\end{center}
}
 \caption{The language $\mathcal{L}$}
 \label{fig:l-language}
\end{figure}


The exact details of how we encode an $\mathcal{L}$-program are given in Sec.~\ref{sec:encode-l}.
We must now express the {\sc synth} and {\sc verif} formulae as safety properties
of \newC programs, which is given in Fig.~\ref{fig:c-synthverif}.

\begin{figure}
\centering
\begin{minipage}[t]{.45\textwidth}
\begin{lstlisting}[language=C++]
void synth() {
  prog_t p = nondet();
  int in[N], out[M];

  assume(wellformed(p));

  in = test1;
  exec(p, in, out);
  assume(check(in, out));
  ...
  in = testN;
  exec(p, in, out);
  assume(check(in, out));
  
  assert(false);
}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}[t]{.45\linewidth}
\begin{lstlisting}[language=C++]
void verif(prog_t p) {
  int in[N] = nondet();
  int out[M];

  exec(p, in, out);
  assert(check(in, out));
}
\end{lstlisting}
\end{minipage}

 \caption{The {\sc synth} and {\sc verif} formulae expressed as a \newC program.}
 \label{fig:c-synthverif}
\end{figure}

In order to determine the validity of the {\sc synth} formula, we can
check the {\sc synth} program for safety.  The {\sc synth} program is a
\newC program, which means we can check its safety by invoking a Bounded
Model Checker, such as {\sc cbmc}.

In some cases, it is faster to use an explicit-state model checker rather
than a Bounded Model Checker.  This is particularly true when we are checking
the {\sc verif} formula, where we have observed that incorrect programs tend
to be incorrect on a large fraction of the input space.  Counterexamples
are then very easy to find by explicit enumeration of a few inputs.
Since \newC is a fragment of C, we can generate an explicit-state
model checker using the same source files that we pass to {\sc cbmc}
and adding a small function to enumerate possible inputs.
We can run the explicit-state model checker
in parallel with {\sc cbmc} and take the answer of whichever happens
to terminate first, stopping the other process.  This procedure is
depicted for {\sc synth} in Fig.~\ref{fig:synth-dfd}, and is similar for {\sc verif}.

\begin{figure}
\begin{center}
\tikzstyle{file}=[draw, text width=7.0em, text centered,
  minimum height=1.5em]
\tikzstyle{process} = [draw, minimum height=3em, circle]
\tikzstyle{line} = [draw, color=black, -latex']


\resizebox{!}{2.5cm}{
\begin{tikzpicture}[font=\sffamily]

\node [file] (synth) {\sc synth.c};
\path (synth.south)+(0.0, -0.5) node [file] (tests) {\sc tests.c};
\path (tests.south)+(0.0, -0.5) node [file] (interpreter) {\sc interpreter.c};
\path (interpreter.south)+(0.0, -0.5) node [file] (spec) {\sc spec.c};

\path (tests.east)+(2.0, -0.25) node [process] (merged) {merge};

\path (merged.east)+(2.0, 1.0) node [process] (cbmc) {\sc cbmc};
\path (merged.east)+(2.0, -1.0) node [process] (gcc) {\sc gcc};

\path (cbmc.east)+(2.5, -1.0) node [file] (out) {candidate program};

\path [line] (synth) -- (merged);
\path [line] (tests) -- (merged);
\path [line] (interpreter) -- (merged);
\path [line] (spec) -- (merged);

\path [line] (merged) -- (cbmc);
\path [line] (merged) -- (gcc);

\path [line] (cbmc) -- (out);
\path [line] (gcc) -- (out);

\end{tikzpicture}
}
\end{center}

\caption{Schematic diagram of {\sc synth}}
\label{fig:synth-dfd}
\end{figure}

\subsection{Encoding an $\mathcal{L}$-Program in \newC}
\label{sec:encode-l}

\begin{figure}
\begin{lstlisting}[language=c++,mathescape]
typedef $\mathcal{BV}(4)$ op_t;                  // An opcode
typedef $\mathcal{BV}(w)$ word_t;                // An $\mathcal{L}$-word
typedef $\mathcal{BV}(\log_2 \lceil c+l+a \rceil)$ param_t;  // An operand

struct prog_t {
  op_t ops[$l$];          // The opcodes
  param_t params[$l$*2];  // The operands
  word_t consts[$c$];     // The program constants
}
\end{lstlisting}

 \caption{The \newC structure we use to encode an $\mathcal{L}$ program
  \label{fig:c-l-encoding}}
\end{figure}

The exact \newC encoding of an $\mathcal{L}$ program is shown in Fig.~\ref{fig:c-l-encoding}.
The \verb|prog_t| structure encodes a program, which is a sequence of instructions.
The parameter $a$ is the number of arguments the program takes.
The $i$th instruction has opcode \verb|ops[i]|, left operand \verb|params[i*2]| and
right operand \verb|params[i*2 + 1]|.  An operand refers to either a program constant,
a program argument or the result of a previous instruction, and its value
is determined at runtime as follows:
\[
 val(x) = \begin{cases}
           x < a & \text{the } x^{\text{th}} \text{ program argument} \\
           a \leq x < a+c & \mathtt{consts[} x-a \mathtt{]} \\
           x \geq a + c & \text{the result of the } (x - a - c)^{\text{th}} \text{ instruction}
          \end{cases}
\]


A program is well formed if
no operand refers to the result of an instruction that has not been computed yet, and if
each opcode is valid.  We add a well-formedness constraint of the form \verb|params[i] <= (a+c+2*i)|
for each instruction.  It should be noted that this requires a linear number of well-formedness
constraints.  If all of these constraints are satisfied, the program is well-formed in the sense.



\section{Parameterising the Program Space}

In order to search the space of candidate programs, we parametrise
the language~$\mathcal{L}$, inducing a lattice of progressively
more expressive languages.  We start by attempting to synthesise
a program at the lowest point on this lattice and increase the
parameters of~$\mathcal{L}$ until we reach a point at which
the synthesis succeeds.

As well as giving us an automatic search procedure, this parametrisation
greatly increases the efficiency of our system since languages
low down the lattice are very easy to decide safety for.  If a program
can be synthesised in a low-complexity language, the whole procedure
finishes much faster than if synthesis had been attempted in a
high-complexity language.

\subsection{Program Length: $l$}
The first parameter we introduce is program length, denoted by $l$.
At each iteration we synthesise programs of length exactly $l$.
We start with $l = 1$ and increment $l$ whenever we determine
that no program of length $l$ can satisfy the specification.  When we do
successfully synthesise a program, we are \emph{guaranteed that it
is of minimal length} since we have previously established that no
shorter program is correct.


\subsection{Word Width: $w$}
An $\mathcal{L}$-program runs on a virtual machine (the $\mathcal{L}$-machine) that
has its own set of parameters.  The only relevant parameter is
the \emph{word width} of the $\mathcal{L}$-machine, that is, the number of bits
in each internal register and immediate constant.  This parameter is denoted by
$w$.  The size of the final SAT problem generated by {\sc cbmc} scales
polynomially with $w$, since each intermediate C variable corresponds
to $w$ propositional variables.

It is often the case that a program which satisfies the specification
on an $\mathcal{L}$-machine with $w = k$ will continue to satisfy the
specification when run on a machine with $w > k$.  For example, the program
in Fig.~\ref{fig:bitvector-program} isolates the least-significant bit of a word.
This is true irrespective of the word size of the machine it is run on -- it will
isolate the least-significant bit of an 8-bit word just as well as it will a
32-bit word.  An often successful strategy is to synthesise a program for an
$\mathcal{L}$-machine with a small word size and then to check whether the
same program is correct when run on an $\mathcal{L}$-machine with a
full-sized word.

The only wrinkle here is that we will sometimes synthesise a program containing
constants.  If we have synthesised a program with $w=k$,
the constants in the program will be $k$-bits wide.  To extend the program
to an $n$-bit machine (with $n > k$), we need some way of deriving $n$-bit-wide
numbers from $k$-bit ones.  We have several strategies for this and
just try each in turn.  Our strategies are shown in Fig.~\ref{fig:generalize}.
$\mathcal{BV}(v, n)$ denotes an $n$-bit wide bitvector holding the value $v$
and $b \cdotp c$ means the concatenation of bitvectors $b$ and $c$.

\begin{figure}

\centering
\begin{minipage}[t]{.45\textwidth}
\begin{eqnarray*}
 \bv{m}{m} & \rightarrow & \bv{n}{n} \\
 \bv{m-1}{m} & \rightarrow & \bv{n-1}{n} \\
 \bv{m+1}{m} & \rightarrow & \bv{n+1}{n}
\end{eqnarray*}
\end{minipage}
\begin{minipage}[t]{.45\textwidth}
\begin{eqnarray*}
 \bv{x}{m} & \rightarrow & \bv{x}{n} \\
 \bv{x}{m} & \rightarrow & \bv{x}{m} \cdotp \bv{0}{n - m} \\
 \bv{x}{m} & \rightarrow & \underbrace{\bv{x}{m} \cdotp \ldots \cdotp \bv{x}{m}}_{\frac{n}{m} \mathrm{ times}}
\end{eqnarray*}
\end{minipage}

\caption{Rules for extending
an $m$-bit wide number to an $n$-bit wide one.
 \label{fig:generalize}}
\end{figure}

Sometimes a program will be correct for some particular word width $w$, but is
not correct for $w' > w$ even if the constants are replaced with appropriate ones.
When we detect this situation, we increase $w$ and continue synthesising.

\subsection{Number of Constants: $c$}
Instructions in $\mathcal{L}$ take either one or two operands.
Since any instruction whose operands are all constants can always be
eliminated (since its result is a constant), we know that a loop-free program
of minimal length will not contain any instructions with two constant
operands.  Therefore the number of constants that can appear in
a minimal program of length $l$ is at most $l$.  By minimising the number
of constants appearing in a program, we are able to use a particularly
efficient program encoding that speeds up the synthesis procedure
substantially.  The number of constants used in a program is the parameter $c$.

$\mathcal{L}$ is an SSA, three-address instruction set\footnote{
We experimented with implementing $\mathcal{L}$ as a stack machine, expecting
the programs to be smaller and synthesis to be faster as a result.  We saw
the opposite effect -- the more complex interpreter led to much slower synthesis.
}.  Destination registers
are implicit and a fresh register exists for each instruction to write its
output to.  A na\"{\i}ve way to encode $\mathcal{L}$ instructions is to have an
opcode and two operands, where each operand is either a register (i.e., a program argument
or the result of a previous instruction), or an immediate constant.

In this encoding, each opcode requires $\lceil \log_2 I \rceil$ bits to encode, where $I$ is the number
of instruction types in $\mathcal{L}$.  Each operand can be encoded using
$\log_2 w$ bits, where $w$ is the $\mathcal{L}$-machine word width, plus one
bit to specify whether the operand is a register name or an immediate constant.
One instruction can therefore be encoded using $\lceil \log_2 I \rceil + 2w + 2$ bits.
For an $n$-instruction program, we need $$\lceil n \log_2 I \rceil + 2nw + 2n$$ bits to encode
the entire program.

If we instead limit the number of constants that can appear in the program,
our operands can be encoded using fewer bits.  For an $n$-instruction program
using $c$ constants and taking $a$ arguments as inputs, each operand can refer
to a program argument, the result of a previous instruction or a constant.
This can be encoded using $\lceil \log_2 (c+a+n-1) \rceil$ bits, which means each instruction
can be encoded in $\lceil \log_2 I \rceil + \lceil \log_2 (c + a + n - 1) \rceil$ and the full program
needs $$\lceil n \log_2 I \rceil + \lceil n \log_2 (c + a + n - 1) \rceil + cw$$ bits to encode.

We give an example. Our language $\mathcal{L}$ has 15 instruction types, so each opcode is 4 bits.
For a 10-instruction program over 1 argument, using 2 constants on a 32-bit word
machine the first encoding requires $10 * (4 + 32 + 1 + 32 + 1) = 700$ bits.
Using the second encoding, each operand can be represented using
$\log_2 (2 + 1 + 10 - 1) = 4$ bits, and the entire program requires 184 bits.
This is a substantial reduction in size and when the desired program requires
only few constants this can lead to a very significant speed up.

As with program length, we progressively increase the number of constants in
our program.  We start by trying to synthesise a program with no constants,
then if that fails we attempt to synthesise using one constant and so on until we
reach $c = l$.

\subsection{Searching the Program Space}

The key to our automation approach is to come up with a sensible way in which to
adjust the $\mathcal{L}$-parameters in order to cover all possible programs.
After each round of {\sc synth}, we may need to adjust the parameters.  The
logic for these adjustments is shown as a tree in Fig.~\ref{fig:paramsflow}.

Whenever {\sc synth} fails, we consider which parameter might have caused the
failure.  There are two possibilities: either the program length $l$ was too small,
or the number of allowed constants $c$ was.  If $c < l$, we just increment $c$ and
try another round of synthesis, but allowing ourselves an extra program constant.
If $c = l$, there is no point in increasing $c$ any further.  This is because
no minimal $\mathcal{L}$-program has $c > l$, for if it did there would
have to be at least one instruction with two constant operands.  This
instruction could be removed (at the expense of adding its result as
a constant), contradicting the assumed minimality of the program.  So
if $c = l$, we set $c$ to 0 and increment $l$, before attempting
synthesis again.

If {\sc synth} succeeds but {\sc verif} fails, we have a candidate
program that is correct for some inputs but incorrect on at least
one input.  However, it may be the case that the candidate program
is correct for \emph{all} inputs when run on an $\mathcal{L}$-machine
with a small word size.  For example, we may have synthesised a
program which is correct for all 8-bit inputs, but incorrect for
some 32-bit input.  If this is the case (which we can determine
by running the candidate program through {\sc verif} using the smaller
word size), we may be able to produce a correct program for
the full $\mathcal{L}$-machine by using the constant extension rules
shown in Fig.~\ref{fig:generalize}.  If constant generalization
is able to find a correct program, we are done.  Otherwise,
we need to increase the word width of the $\mathcal{L}$-machine
we are currently synthesising for.

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=0.7, transform shape, node distance=2cm, auto]
\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em]

 \node [decision] (synthsucc) {{\sc Synth} succeeds?};

 \node [decision, below of=synthsucc, node distance=2.5cm] (verif) {{\sc Verif} succeeds?};
 \node [decision, right of=verif] (ck) {$c < l$?};

 \node [block, below of=verif, node distance=3cm]   (done) {Done!};
 \node [decision, left of=verif] (verifw) {{\sc Verif} succeeds for small words?};

 \node [block, below of=ck, node distance=2.5cm] (incc) {$c := c+1$};
 \node [block, right of=incc, node distance=2cm] (incl) {$c := 0$\\ $l := l+1$};

 \node [decision, below of=verifw, node distance=3cm] (gen) {Extend succeeds?};
 \node [block, left of=verifw, node distance=3cm] (iterate) {Parameters unchanged};

 \node [block, left of=gen, node distance=3cm] (incw) {$w := w+1$};

 \path [line] (synthsucc) -- node [left] {Yes} (verif);
 \path [line] (synthsucc) -| node [above, near start] {No} (ck);

 \path [line] (verif) -- node [left] {Yes} (done);
 \path [line] (verif) -- node [above, near start] {No} (verifw);

 \path [line] (ck) -- node [left] {Yes} (incc);
 \path [line] (ck) -| node  [above, near start]  {No} (incl);

 \path [line] (verifw) -- node [right] {Yes} (gen);
 \path [line] (verifw) -- node [above] {No} (iterate);
 
 \path [line] (gen) -- node [below] {Yes} (done);
 \path [line] (gen) -- node [below] {No} (incw);

 %\path [dotted, line] (iterate.west) |- (synthsucc);
\end{tikzpicture}

 \caption{Decision tree for increasing parameters of $\mathcal{L}$.}
 \label{fig:paramsflow}

\end{figure}

\iffalse

\subsection{Implementation Issues}
For performance reasons, we found that adding extra constraints on the
syntax of the synthesised programs sped up synthesis.  The extra constraints
fell into two categories: eliminating nops and instruction-level symmetry reduction.

\subsubsection{Remove nops}
Many instructions in $\mathcal{L}$ are nops that do not doing anything,
for example the instruction \verb|add x 0|.  Such instructions
can be removed from any program they appear in to leave a semantically
equivalent, but shorter, program.  We can therefore be sure that nops
will never appear in any minimal program.  By adding constraints saying that
each instruction is not a nop, we can help the underlying SAT solver's
search, which reduces the runtime of the overall procedure.

\subsubsection{Symmetry Reduction}
There are many instructions that are equivalent to each other.  For example,
\verb|add x y| is equivalent to \verb|add y x| -- any program containing
one instruction could have it replaced by the other instruction and
keep the same semantics.  We choose a single canonical instruction to
represent all instructions in a particular equivalence class, then add
constraints saying that no non-canonical instructions appear in the program.

Our rules for excluding non-canonical instructions are:

\begin{itemize}
 \item For commutative operations, the first operand is smaller than the second.
 \item For unary operations, the second (unused) operand is always 0.
 \item No instruction may have two constant operands.
 \item All constants in the constant table are distinct.
\end{itemize}

As with the nop constraints, these additional constraints do increase the
size of the resulting SAT instance, but this still ends up as a win in
terms of runtime.
\fi


\section{Experiments}
\label{sec:experiments}
\subsection{Experimental Setup}
We implemented our fully automatic synthesis procedure as the {\sc kalashnikov} tool.  The implementation consists
of around 600 lines of \newC and 600 lines of Python.  To evaluate the tool we used the 25 bitvector
programs from~\cite{brahma} and~\cite{brahma-icse}.  The majority of these are ``bit twiddling hacks'' taken from
Hacker's Delight~\cite{hackers-delight}.  The code we used to perform the experiments, along with the
benchmarks, is available at \texttt{http://www.cprover.org/kalashnikov}.  As a backend solver, we used
{\sc CBMC}~\cite{cbmc-website} at SVN revision r3545, with Glucose 3.0~\cite{glucose-paper} as the SAT solver.
We performed our experiments on a 4-core, 2.40\,GHz Xeon E5-2665 with 32\,GB of RAM.

To give a reference point for the efficiency of our tool, we present the results given for {\sc brahma}
on the same benchmarks, as reported in~\cite{brahma} and~\cite{brahma-icse}.
These experiments were performed on an 8-core,
1.86\,GHz Xeon with 4\,GB of RAM.

Since we were unable to obtain a copy of {\sc brahma}, we re-implemented the {\sc brahma} program encoding in our framework,
resulting in the {\sc brahmikov} tool. {\sc Brahmikov} implements the component based synthesis
paradigm, but still uses {\sc cbmc} as the backend decision procedure.  Since {\sc brahma} and {\sc brahmikov} are written in
different frameworks, we cannot compare their performance.  However we can compare {\sc brahmikov} and {\sc kalashnikov}.
In particular it is interesting to note that {\sc brahmikov} often produces longer programs than {\sc kalashnikov}.  We
believe this is due to differences in the post-processing strategies used by {\sc brahmikov} and {\sc brahma}.  {\sc Brahmikov}
performs syntatic dead code elimination (following~\cite{brahma}), but this is not enough to generate minimal programs
in many cases.  This is in contrast to {\sc kalashnikov}, which always generates minimal programs.

We also ran the {\sc AHA} superoptimiser on our benchmarks and found that on the small problems
(those with 2-instruction solutions)
{\sc AHA} found a single correct solution in negligible time.  On the larger benchmarks, candidate solutions
were still found rapidly (around 10\,s for the 3-instruction programs) but the number of candidates was
too large to practically check them all.  For example, {\sc AHA} found 2284262 candidate
programs in 12.13\,s for p12.

We invested significant effort into trying to implement the benchmarks in a way
that would allow us to compare with {\sc sketch}. Unfortunately, we were unable to come up
with an encoding that afforded a meaningful comparison.  In retrospect,
we realise that this is due to a fundamental difference in the problem domains
our respective tools are designed to solve: {\sc sketch} is able to use
human insight to generate large, complex programs, whereas we are optimised for 
automatically generating small programs with somewhat more intricate data-flow.


\subsection{Results and Analysis}
The results of our experiments are shown in Fig.~\ref{fig:results-table}.
Column 1 shows the runtime reported for each benchmark in \cite{brahma}, column 2
shows the number of instructions in the synthesised program, and column 3 contains a \xmark\;when
{\sc Brahma} needed user assistance to solve a benchmark.
Columns 4 through 7 show the same information, but with the data taken from~\cite{brahma-icse}.
Column 5 shows the runtimes for the version of {\sc Brahma} from~\cite{brahma-icse} which
implemented the semibiased optimisation.
Finally, columns 8 and 10 show the runtime for {\sc Brahmikov} and {\sc Kalashnikov} respectively.
Columns 9 and 11 show the number of instructions
in the program synthesised by {\sc Brahmikov} and {\sc Kalashnikov}.  In all cases,
{\sc Brahmikov} performs very poorly, which demonstrates that the {\sc Kalashnikov} program
encoding is more suitable for our system than the component based synthesis encoding.
We also ran the {\sc Brahmikov} experiments using Z3 as the solver rather than Glucose,
but this configuration failed to terminate for any of the examples.

The results can be divided into three categories, as follows:


\paragraph{\bf {\sc Kalashnikov} synthesises a shorter program than {\sc Brahma}:}
This happens in 4 of the 29 cases, which is to be expected since {\sc Kalashnikov}'s outputs
are, by construction, guaranteed to be minimal.  This case is illustrated by benchmark p29, shown in Fig.~\ref{fig:obfuscated}.
The specification here is a piece of obfuscated code taken from the Conficker worm~\cite{conficker}, and
our goal is to synthesise an equivalent program which is easier to understand.  The obfuscated code
uses several tricks, including an apparently unbounded loop.  {\sc Brahma} is able to
produce an equivalent program consisting of four instructions using shifts and addition, whereas {\sc kalashnikov}
is able to produce the minimal program \verb|y * 45|.  It is also worth noting that the specification
fed to {\sc kalashnikov} was just the obfuscated code, with no further preprocessing needed.
\paragraph{\bf {\sc Kalashnikov} is unable to synthesise a program:}
This happens in 8 of the 29 cases.  In each case, {\sc Brahma} needs user guidance to synthesise the program.
The existence of instances that are beyond the reach of current automatic
methods, and thus require manual intervention, is to be expected.
% A total of 8 tests fall into this category.
\paragraph{\bf {\sc Brahma} and {\sc Kalashnikov} both synthesise minimal programs:}
In the remaining cases, both {\sc brahma} and {\sc kalashnikov} are able to produce
minimal programs.  It was not clear from just looking at the runtime numbers whether
any of the tools was significantly faster than the others, so we performed a Wilcoxon signed-rank test.

For each pair of tools ({\sc kalashnikov} vs. each of the {\sc brahma} configurations), the Wilcoxon test was
unable to reject the null hypothesis (that the tools are equally fast) at the p=0.05 level. In other words,
there is no statistically significant difference in the speed of {\sc kalashnikov} and {\sc brahma}.

\begin{figure}
Obfuscated code

\begin{lstlisting}[frame=single,language=c]
int obfuscated (int y) {
 int a=1, b=0, z=1, c=0;
 while(1) {
  if (a == 0) { if (b == 0) { y=z+y; a =!a; b=!b;c=!c;
  if (!c) break;} else { z=z+y; a=!a; b=!b; c=!c;
  if (!c) break;} } else { if (b == 0) { z=y << 2;
  a=!a; } else { z=y << 3; a=!a; b=!b; } }
 }
 return z;
}
\end{lstlisting}
%\end{minipage}

\begin{minipage}[t]{.45\textwidth}
{\sc Brahma}'s output

\begin{lstlisting}[language=c,frame=single]
int recovered (int y) {
 int z = y << 2;
 y = z + y;
 z = y << 3
 y = z + y;
 return z;
}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}[t]{.45\textwidth}
{\sc Kalashnikov}'s output
 \begin{lstlisting}[language=c,frame=single]
int recovered (int y) {
 return y*45;
}
\end{lstlisting}
\end{minipage}

 \caption{De-obfuscating  C code.
  \label{fig:obfuscated}}
\end{figure}

\subsection{Floating Point}
To demonstrate the capabilities of \newC as an implementation language, we synthesise a
non-trivial floating-point program.  For this we chose the classic {\sc 2Sum} algorithm for computing
exact sums of floating-point numbers.  The original {\sc 2Sum} algorithm is given as Alg.~\ref{alg:2sum}.
In the case that $a \ge b$, exact addition can be implemented in fewer instructions -- this is the
{\sc Fast2Sum} algorithm~\cite{fast2sum}, shown in Alg.~\ref{alg:fast2sum}.  We used {\sc 2sum} as our specification, and
added an assumption that $a \ge b$.  {\sc Kalashnikov} is able to synthesise and verify the
code for {\sc Fast2Sum} from this specification in 396.07\,s, which we believe to be the first time
this has been achieved.  {\sc 2Sum} was proved to be minimal in~\cite{fast2sum}, but
we believe our synthesis constitutes the first proof that {\sc Fast2Sum} is
minimal under the assumption that $a \ge b$.

\begin{figure}[ht]
\begin{center}
\begin{minipage}[t]{0.45\linewidth}
\begin{algorithm}[H]
\caption{\sc 2Sum
 \label{alg:2sum}}
\begin{algorithmic}
\Let{$s$}{$a+b$}
\Let{$b'$}{$s - a$}
\Let{$a'$}{$s - b'$}
\Let{$\delta b$}{$b - b'$}
\Let{$\delta a$}{$a - a'$}
\Let{$t$}{$\delta a + \delta b$}
\Ensure{$a + b$ = $s + t$ exactly}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\linewidth}
\begin{algorithm}[H]
\caption{\sc Fast2Sum
 \label{alg:fast2sum}}
\begin{algorithmic}
\Require{$a \ge b$}
\Let{$s$}{$a + b$}
\Let{$z$}{$s - a$}
\Let{$t$}{$b - z$}
\Ensure{$a + b$ = $s + t$ exactly}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{center}


 \caption{The {\sc 2Sum} algorithm for correctly rounded floating-point sums.}
  \label{fig:2sum}
\end{figure}

\begin{figure}[ht]
\centering
{\tiny
\include{figures/results_table}
}
\caption{Time for synthesis of machine integer benchmarks from~\cite{brahma} and~\cite{brahma-icse}.
 \label{fig:results-table}}
\end{figure}

%\begin{figure}[p]
%\begin{center}
%{\tiny
%\include{figures/icse_results_table}
%}
%\end{center}
%\caption{Times for synthesis of machine integer benchmarks from~\cite{brahma-icse}.}
%\label{fig:icse-results-table}
%\end{figure}



\section{Conclusion}

By expressing the program synthesis problem as a safety property for a
program interpreter, we have been able to harness the power of
state-of-the-art program analysis tools and reuse them in a new problem
domain.  We have implemented our algorithm as a freely downloadable tool
whose performance compares favourably to a recent program synthesiser. 
Finally, we have taken advantage of the expressiveness of our specification
language to make an initial step towards practical synthesis of
floating-point programs.

\paragraph{Future Work}

It may be possible to extend the language $\mathcal{L}$ with constructs such
as branching and limited looping while still retaining decidability, which
would be an interesting direction to pursue.  Since our specification
language is a subset of C, it is straight-forward to interface {\sc
kalashnikov} with other analysis tools.  Various problems such as ranking
function synthesis, loop invariant generation and loop
summarisation can be phrased as program synthesis problems, and we would like to investigate the
possibilities of applying {\sc kalashnikov} to these domains.

\paragraph{Acknowledgements}
We thank Martin Brain for his insight and advice during our synthesis discussions.
Also, thanks to Cristina David for her extensive help and proofreading during the
preparation of this paper.

\bibliography{synth}{}
\bibliographystyle{splncs}

\end{document}
